# Transformers 처음부터 만들어보기

## Input Embedding이란?
텍스트를 임베딩 벡터로 변환하는 과정. 트랜스포머에서는 512 크기의 임베딩 벡터로 바꾼다.

## Positional Embedding이란?
각 단어가 해당 문서에서 위치한 절대적 위치를 임베딩한 것. 딱 한번만 계산된 후, 학습이나 추론 과정에서 사용된다.

### PyTorch 문법 정리
- torch.zeros(x, y) : x행 y열짜리 0으로 채운 행렬 반환
- torch.arange(x, y) : x부터 y까지 일정한 간격의 텐서 반환
- torch.arange().unsqueeze(i) : 텐서에 새로운 차원을 추가하는 함수.
  - i=0: 0번째 위치에 차원 추가. 주로 **배치를 만들고 싶을 때** 사용
  - i=1: 1번째 위치에 차원 추가. 주로 **특정 차원을 늘려야 할 때** 사용