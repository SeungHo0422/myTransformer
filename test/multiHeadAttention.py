import torch
import torch.nn as nn
import math

from model import MultiHeadAttentionBlock  # í•„ìš” ì‹œ import

def test_multihead_attention():
    # í…ŒìŠ¤íŠ¸ ì„¤ì •
    batch_size = 2
    seq_len = 5
    d_model = 512
    num_heads = 8
    dropout = 0.1

    # ê°€ì§œ ì…ë ¥ ìƒì„± (batch_size, seq_len, d_model)
    x = torch.randn(batch_size, seq_len, d_model)

    # ë§ˆìŠ¤í¬ (í•„ìš” ì—†ìœ¼ë©´ None ê°€ëŠ¥)
    mask = torch.ones(batch_size, 1, 1, seq_len)  # broadcast ê°€ëŠ¥ í˜•íƒœ (B, 1, 1, L)

    # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
    mha = MultiHeadAttentionBlock(d_model, num_heads, dropout)

    # Forward íŒ¨ìŠ¤ (Q=K=V)
    out = mha(x, x, x, mask)

    # ê²°ê³¼ í™•ì¸
    print("âœ… Output shape:", out.shape)  # (batch_size, seq_len, d_model)
    assert out.shape == (batch_size, seq_len, d_model), "Output shape mismatch"

    print("âœ… Attention scores shape:", mha.attention_scores.shape)  # (B, h, L, L)
    assert mha.attention_scores.shape == (batch_size, num_heads, seq_len, seq_len), "Attention scores shape mismatch"

    print("ğŸ‰ í…ŒìŠ¤íŠ¸ í†µê³¼")


def test_attention_embedding_change():
    batch_size = 2
    seq_len = 5
    d_model = 512
    num_heads = 8
    dropout = 0.0  # ì°¨ì´ ë¶„ì„í•  ë•ŒëŠ” dropout ì—†ì´

    # ê°€ì§œ ì…ë ¥ (Q = K = V)
    x = torch.randn(batch_size, seq_len, d_model)

    # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
    mha = MultiHeadAttentionBlock(d_model, num_heads, dropout)

    # Forward ìˆ˜í–‰
    output = mha(x, x, x, mask=None)

    # 1. Shape ì²´í¬
    assert output.shape == x.shape, "Shape mismatch"

    # 2. ì„ë² ë”© ë³€í™” í™•ì¸ (ì˜ˆ: Cosine Similarity / Norm ì°¨ì´)
    cosine_sim = nn.functional.cosine_similarity(x, output, dim=-1)  # (B, L)
    l2_diff = torch.norm(x - output, dim=-1)                         # (B, L)

    print("ğŸ“ í‰ê·  Cosine Similarity (ì…ë ¥ vs ì¶œë ¥):", cosine_sim.mean().item())
    print("ğŸ“ í‰ê·  L2 ì°¨ì´ (ì…ë ¥ vs ì¶œë ¥):", l2_diff.mean().item())

    # 3. ì‹œê°ì  ê²°ê³¼
    for b in range(batch_size):
        for i in range(seq_len):
            print(f"Token {i+1} in batch {b+1}:")
            print(f"  CosSim = {cosine_sim[b, i]:.4f}, L2 Diff = {l2_diff[b, i]:.4f}")

# ì‹¤í–‰
test_multihead_attention()
test_attention_embedding_change()